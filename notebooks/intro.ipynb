{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Reinforcement Learning -- Intro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Most of human and animal learning is unsupervised learning. If intelligence was a cake, *unsupervised learning* would be the cake, *supervised learning* would be the icing on the cake, and *reinforcement learning* would be the cherry on the cake. </p>\n",
    "Yann LeCun"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/cake.jpg\" width=\"500px\"></img>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The origins of *reinforcement learning* go back at least until the seminal contributions of [Ian H. Witten](https://mathscinet.ams.org/mathscinet/search/publdoc.html?pg1=INDI&s1=211968&r=8) in the 1970s. However, for a long time it has been considered primarily within academia as real-world problems were out of reach for a long time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This changed dramatically with the advent of deep learning in the last decade. This notebook first gives an overview on the achievements in different application areas in the recent past. Then, we describe briefly and in loose terms the main concepts in the field. Later, we will come back to the individual topics in more detail. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Robots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/roomba.jpg\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The primary industrial application area of reinforcement learning lies in robotics. At the moment robots are predominantly restricted to carrying out well-defined, repeatable tasks. Since robots often come with many degrees of freedom, writing programs to carry out a delicate task often consumes a humongous amount of time.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reinforcement learning holds the promise of bringing within reach a variety of more complex tasks that may change from one instance to the next. For instance, this may involve detecting a specific object, picking it up and placing it in another location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video width=\"640\" height=\"480\" controls>\n",
       "  <source src=\"images/dexnet.mp4\" type=\"video/mp4\">\n",
       "</video>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%HTML\n",
    "<video width=\"640\" height=\"480\" controls>\n",
    "  <source src=\"images/dexnet.mp4\" type=\"video/mp4\">\n",
    "</video>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reinforcement learning for robotics involves challenges that have yet to be addressed accordingly, both from a theoretical as well as practical perspective. For instance, such systems need to be highly sample efficient. While with sufficient compute power, a program could run through millions of chess games in the fraction of a time that a human would require, such speed-ups are not possible in physical systems. Additionally the set of exploration strategies needs to be severely constrained, since we want to avoid for instance a drone that crashes while exploring a purportedly promising strategy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Games"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Atari"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a 2015 [nature paper](https://www.bbc.com/news/technology-48959931), Google DeepMind invents **Deep Q-Learning**, a technique combining Q-learning with deep convolutional networks. With this technique, it is possible to train an agent into reaching super-human performance in a variety of Atari Games -- including of course *Space Invaders*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video width=\"640\" height=\"480\" controls>\n",
       "  <source src=\"images/breakout.mp4\" type=\"video/mp4\">\n",
       "</video>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%HTML\n",
    "<video width=\"640\" height=\"480\" controls>\n",
    "  <source src=\"images/breakout.mp4\" type=\"video/mp4\">\n",
    "</video>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Go"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a 2016 match against Lee Sedol, Google DeepMind achieved the next success through [AlphaGo](https://www.nature.com/articles/nature16961), the first *Go*-program to beat a 9-dan professional player. It enhances *Monte-Carlo tree search* through *value and policy networks*, both based on deep convolutional networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/alphago.jpg\"></img>\n",
    "https://blog.floydhub.com/robotic-arm-control-deep-reinforcement-learning/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Montezuma's Revenge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In 2018, Uber releases [Go-Explore](https://eng.uber.com/go-explore/), an AI to reach super-level performance on the notriously difficult game *Montezuma's Revenge*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video width=\"640\" height=\"480\" controls>\n",
       "  <source src=\"images/montez.mp4\" type=\"video/mp4\">\n",
       "</video>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%HTML\n",
    "<video width=\"640\" height=\"480\" controls>\n",
    "  <source src=\"images/montez.mp4\" type=\"video/mp4\">\n",
    "</video>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dota 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In 2019, the [OpenAI Five](https://openai.com/blog/how-to-train-your-openai-five/) agent was the first to win the international finals of the game Dota 2. The challenge of this task is that Dota 2 requires sophisticated coordination between five players. The agents are based on 4096-unit LSTM networks. The major finding was that decisive improvements are possible by training existing architectures on unprecedented scales. The final agent was trained on an equivalent of 180 years of games per day."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/dota.png\"></img>\n",
    "https://openai.com/blog/how-to-train-your-openai-five/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Poker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, in July 2019, Facebook developed the AI [Pluribus](https://science.sciencemag.org/content/early/2019/07/10/science.aay2400) achieving superhuman performance in 6-player poker game. This problem is challenging, since as in the case of Dota 2, Poker is a game featuring imperfect information and requiring strong cooperation among the players."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/pluribus.jpg\"></img>\n",
    "https://www.bbc.com/news/technology-48959931"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the remainder, we quickly go informally through the main concepts underpinning deep reinforcement learning. We discuss all of them in full detail in a formal setting later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All instances of reinforcement learning can be cast to a setting where an **agent** interacts with the **environment**. This happens by choosing **actions** according to some **policy**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For instance, we may consider a space-ship as an agent and the Atari-simulator as the environment. At each time step, the space ship can choose among one of three possible actions: \n",
    "\n",
    "1. Move to the left\n",
    "2. Move to the right\n",
    "3. Fire the laser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each action comes with two consequences. It changes the **state** of the environment and the agent may receive a **reward**. The goal of the agent is to find a policy maximizing the rewards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
