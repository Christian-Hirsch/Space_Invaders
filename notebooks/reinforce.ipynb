{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REINFORCE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous notebook, we looked at [Deep Q Network](./dqn.ipynb) and [Double Deep Q Networks](./ddqn.ipynb) as methods to estimate the $Q$-function by leveraging the Bellman equation. In the present notebook, we meet the [REINFORCE algorithm](https://link.springer.com/article/10.1007/BF00992696), which makes it possible to learn optimal policies directly by performing gradient descent on the policy parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To this end, we build on the code samples from the amazing [pytorch-examples repository](https://github.com/pytorch/examples/blob/master/reinforcement_learning/reinforce.py) and follow closely the highly instructive conceptual derivations from [OpenAI spinning up](https://spinningup.openai.com/en/latest/spinningup/rl_intro3.html#)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Policy Gradient Theorem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the **Policy Gradient** approach, our goal is to find a parametric policy $\\pi_\\theta$ maximizing the expected total rewards $J(\\pi_\\theta) = \\mathbb{E}_{\\tau \\sim \\pi_\\theta}[R(\\tau)]$ under the policy $\\pi_\\theta$, where $R(\\tau)$ denotes the total reward along a trajectory $\\tau$ sampled according to the policy $\\pi_\\theta$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The basic idea of the **REINFORCE algorithm** or policy gradient methods is to update the policy parameters $\\pi_\\theta$ via gradient descent. To that end, we need to compute the gradient $\\nabla_\\theta J(\\pi_\\theta)$ of the expected total rewards $J(\\pi_\\theta)$ with respect to $\\theta$. That is, we put \n",
    "$$\\theta_{k + 1} = \\theta_k + \\alpha \\nabla_\\theta J(\\pi_\\theta)$$\n",
    "for a suitable learning rate $\\alpha > 0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compute the gradient, $\\nabla_\\theta J(\\pi_\\theta)$ we let $p_\\theta(\\tau)$ denote the probability density of sampling a trajectory $\\tau$ when following the parametric policy $\\pi_\\theta$.  Then, we reexpress $\\nabla_\\theta J(\\pi_\\theta)$ via the **log-derivative trick**\n",
    "\\begin{align}\n",
    "\\nabla_\\theta J(\\pi_\\theta) &= \\nabla_\\theta \\int_\\tau R(\\tau)p_\\theta(\\tau) \\mathrm d \\tau \n",
    "= \\int_\\tau \\nabla_\\theta R(\\tau)p_\\theta(\\tau) \\mathrm d \\tau \n",
    "=\\int_\\tau R(\\tau)\\nabla_\\theta\\log (p_\\theta(\\tau))  p_\\theta(\\tau) \\mathrm d \\tau \n",
    "= \\mathbb E_{\\tau \\sim \\pi_\\theta}\\big[\\nabla_\\theta R(\\tau)\\log (p_\\theta(\\tau)) \\big]\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we compute the probability of observing a trajectory $\\tau = (s_0, s_1, \\dots, s_T)$ of length $T \\ge 1$ when starting at a state $s_0$ and following the policy $\\pi_\\theta$. This probability factorizes as \n",
    "$$p_\\theta(\\tau) = \\prod_{t < T} p(s_{t + 1}\\,|\\,s_t, a_t) \\pi_\\theta(a_t|s_t),$$\n",
    "where $p(s_{t + 1}\\,|\\, s_t, a_t)$ denotes the transition probability of the environment transitions from state $s_t$ to state $s_{t + 1}$ when performing the action $a_t$. Since this probability does not depend on the parameter $\\theta$, we conclude that \n",
    "$$\\nabla_\\theta \\log (p_\\theta(\\tau)) = \\sum_{t < T} \\nabla_\\theta \\log \\pi_\\theta(a_t|s_t).$$\n",
    "Hence, we obtain the **Policy-Gradient Theorem**\n",
    "\\begin{align*}\n",
    "\\nabla_\\theta J(\\pi_\\theta) &= \\mathbb E_{\\tau  \\sim \\pi_\\theta}\\Big[ R(\\tau) \\sum_{t < T} \\nabla_\\theta \\log \\pi_\\theta(a_t|s_t)\\Big].\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loosely speaking, we increase the probability of an action $a_t$ in state $s_t$ by taking a gradient step with respect to the log-probability scaled with the total reward."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To summarize, the log-derivative trick made it possible to render the problem of finding the parameters for the optimal policy amenable the powerful gradient descent technique known from supervised learning. However, as noted in the [OpenAI tutorial](https://spinningup.openai.com/en/latest/spinningup/rl_intro3.html), there are two fundamental differences to the supervised setting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. In stochastic gradient descent, we compute gradients on samples taken from a fixed data distribution. In contrast, in the REINFORCE algorithm, we sample from the policy $\\pi_\\theta$, so that the sampling distribution changes as we update $\\theta$.\n",
    "\n",
    "2. It is tempting to view $-\\mathbb E_{\\pi_\\theta}\\Big[ R(\\tau) \\sum_{t < T}  \\log \\pi_\\theta(a_t|s_t)\\Big]$ as a loss function and track how quickly it decreases over several iterations. However, since the expectation is taken with respect to the $\\theta$-dependent distribution $\\pi_\\theta$, its gradient is not related to $\\nabla_\\theta J(\\pi_\\theta)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## REINFORCE Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we replace the expectation $\\mathbb E_{\\pi_\\theta}$ by an average over Monte Carlo samples from an agent following the policy $\\pi_\\theta$, then it is straightforward to implement the [REINFORCE Algorithm](https://link.springer.com/article/10.1007/BF00992696) for optimizing the policy $\\pi_\\theta$.\n",
    "\n",
    "1. Draw a mini-batch of sample trajectories $\\tau^1, \\dots, \\tau^N$ from $\\pi_\\theta$\n",
    "2. Compute the rewards $R(\\tau^1), \\dots, R(\\tau^N)$\n",
    "3. Compute the gradient step\n",
    "$$\\theta' = \\theta + \\alpha \\frac1N \\sum_{i \\le N} R(\\tau^i)\\sum_{t < T}\\nabla_\\theta \\log(\\pi_\\theta(a_t^i|s_t^i)).$$\n",
    "4. Replace $\\theta \\leftarrow \\theta'$ and go back to 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## REINFORCE Algorithm for the Cartpole Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we illustrate how the REINFORCE Algorithm is applied to the ``CartPole`` example. First, we load the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import torch\n",
    "\n",
    "env = gym.make('CartPole-v1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define the policy network as a multilayer perceptron with one hidden layer. The policy network takes a sample from the state space as input and returns logits for the actions of moving to the left or to the right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "nactions = env.action_space.n\n",
    "state_dim = env.observation_space.shape[0]\n",
    "nhidden = 128\n",
    "p_dropout = .6\n",
    "\n",
    "class Policy(nn.Module):\n",
    "    \"\"\"Neural network parametrizing the policy\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize the policy network\n",
    "        \"\"\"\n",
    "        super(Policy, self).__init__()\n",
    "        self.affine1 = nn.Linear(state_dim, nhidden)\n",
    "        self.dropout = nn.Dropout(p = p_dropout)\n",
    "        self.affine2 = nn.Linear(nhidden, nactions)\n",
    "\n",
    "        #save log-probs for the application in the REINFORCE module later\n",
    "        self.saved_log_probs = []\n",
    "        self.rewards = []\n",
    "\n",
    "    def forward(self, s):\n",
    "        \"\"\"Compute action from state\n",
    "    \n",
    "        # Arguments\n",
    "            s: input state\n",
    "        # Result\n",
    "            suggested action\n",
    "        \"\"\"\n",
    "        s = self.affine1(s)\n",
    "        s = self.dropout(s)\n",
    "        s = F.relu(s)\n",
    "        action_scores = self.affine2(s)\n",
    "        return F.softmax(action_scores, dim = 1)\n",
    "    \n",
    "policy = Policy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To draw an action from the current policy at a state $s$, we send the state $s$ through the policy network and sample from the corresponding probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.distributions import Categorical\n",
    "\n",
    "def select_action(s):\n",
    "    \"\"\"Select action according to policy\n",
    "    \n",
    "    # Arguments\n",
    "        s: state\n",
    "    # Result\n",
    "        selected action\n",
    "    \"\"\"\n",
    "    s = torch.from_numpy(s).float().unsqueeze(0)\n",
    "    probs = policy(s)\n",
    "    m = Categorical(probs)\n",
    "    a = m.sample()\n",
    "\n",
    "    policy.saved_log_probs.append(m.log_prob(a))\n",
    "    return a.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Each time after finishing an environment, we apply the REINFORCE algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def finish_episode():\n",
    "    \"\"\"Apply optimizer to policy network after each episode\n",
    "    \"\"\"\n",
    "    R = 0\n",
    "    policy_loss = []\n",
    "\n",
    "\n",
    "    for r in policy.rewards[::-1]:\n",
    "        R = r + GAMMA * R\n",
    "\n",
    "    for log_prob in policy.saved_log_probs:\n",
    "        policy_loss.append(-log_prob * R)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    policy_loss = torch.cat(policy_loss).sum()\n",
    "    policy_loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    del policy.rewards[:]\n",
    "    del policy.saved_log_probs[:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When running the REINFORCE algorithm,  we see that  initially the rewards increase. However, we also notice substantial fluctuations in the course of the optimization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "running_reward = 10\n",
    "MAX_STEPS = int(1e3)\n",
    "PRINT_FREQ = int(1e3)\n",
    "MA_WEIGHT = .05\n",
    "\n",
    "def reinforce(env,\n",
    "             neps = int(1e4),\n",
    "             init_reward = 10):\n",
    "    \"\"\"Apply reinforce algorithm for several episodes\n",
    "    \n",
    "    # Arguments\n",
    "        env: environment\n",
    "        neps: number of episodes\n",
    "        init_reward: initialization of running reward\n",
    "    \"\"\"\n",
    "    running_reward = init_reward\n",
    "    \n",
    "    for i in range(neps):\n",
    "        s, ep_reward = env.reset(), 0\n",
    "\n",
    "        for _ in range(MAX_STEPS):  \n",
    "            #perform step in environment\n",
    "            a = select_action(s)\n",
    "            s, r, d, _ = env.step(a)\n",
    "            \n",
    "            #collect rewards\n",
    "            policy.rewards.append(r)\n",
    "            ep_reward += r\n",
    "            if d:\n",
    "                break\n",
    "\n",
    "        running_reward = MA_WEIGHT * ep_reward + (1 - MA_WEIGHT) * running_reward\n",
    "        if i % PRINT_FREQ == 0: print(running_reward)\n",
    "        finish_episode()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reward-to-Go Policy Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned above, in the policy-gradient theorem\n",
    "\\begin{align*}\n",
    "\\nabla_\\theta J(\\pi_\\theta) &= \\mathbb E_{\\tau  \\sim \\pi_\\theta}\\Big[ R(\\tau) \\sum_{t < T} \\nabla_\\theta \\log \\pi_\\theta(a_t|s_t)\\Big],\n",
    "\\end{align*}\n",
    "the gradient of the log-probability of an action is scaled by the total reward along a trajectory. However, this is counter-intuitive. Indeed, the probability of taking an action $a_t$ at time $t$ should not depend on rewards before time $t$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make this precise, we expand $R(\\tau)$ as $R(\\tau) = \\sum_{t' < T} \\gamma^{t'} r_{t'}$, where $\\gamma \\in (0,1)$ is the discount factor and $r_{t'} = r(s_{t'}, a_{t'})$ denotes the reward obtained by applying the action $a_{t'}$ in state $s_{t'}$. Hence,\n",
    "\\begin{align*}\n",
    "\\nabla_\\theta J(\\pi_\\theta) &=  \\sum_{t < T}  \\sum_{t' < T} \\mathbb E_{\\tau \\sim \\pi_\\theta}\\Big[ \\gamma^{t'}r_{t'}\\nabla_\\theta \\log \\pi_\\theta(a_t|s_t)\\Big].\n",
    "\\end{align*}\n",
    "We claim that in this double-sum, the terms with $t' < t$ vanish."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indeed, conditioning on $s_{t'}$ and $a_{t'}$ gives that \n",
    "\\begin{align*}\n",
    "\\mathbb E_{\\tau \\sim \\pi_\\theta}\\Big[ r_{t'}\\nabla_\\theta \\log \\pi_\\theta(a_t|s_t)\\Big] &=\n",
    "\\int r(s_{t'}, a_{t'}) \\nabla_\\theta \\log \\pi_\\theta(a_t|s_t) p_\\theta(s_{t'}, a_{t'}, s_t, a_t) \\mathrm d \\tau\\\\\n",
    "&= \\int r(s_{t'}, a_{t'}) \\Big(\\int \\nabla_\\theta \\log(\\pi_\\theta(a_t|s_t))p_\\theta(s_t, a_t| s_{t'}, a_{t'}) \\mathrm d s_{t} \\mathrm d a_{t} \\Big)p_\\theta(s_{t'}, a_{t'}) \\mathrm d s_{t'} \\mathrm d a_{t'}\\\\\n",
    "&= \\int r(s_{t'}, a_{t'}) \\Big(\\int \\Big(\\int\\nabla_\\theta \\log(\\pi_\\theta(a_t|s_t)) \\pi_\\theta(a_t|s_t)\\mathrm d a_{t}\\Big)p_\\theta(s_t| s_{t'}, a_{t'}) \\mathrm d s_{t}  \\Big)p_\\theta(s_{t'}, a_{t'}) \\mathrm d s_{t'} \\mathrm d a_{t'}.\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying the log-derivative trick gives and using that $\\pi_\\theta(\\cdot |s_t)$ is a probability distribution, we arrive at\n",
    "$$\\int\\nabla_\\theta \\log\\pi_\\theta(a_t|s_t) \\pi_\\theta(a_t|s_t)\\mathrm d a_t =\\int\\nabla_\\theta \\pi_\\theta(a_t|s_t) \\mathrm d a_t = \\nabla_\\theta\\int \\pi_\\theta(a_t|s_t) \\mathrm d a_t = \\nabla_\\theta1 = 0.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hence, we obtain the **Reward-to-Go Policy Gradient Theorem**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align*}\n",
    "\\nabla_\\theta J(\\pi_\\theta) &= \\mathbb E_{\\tau \\sim \\pi_\\theta}\\Big[\\sum_{t < T} \\nabla_\\theta \\log \\pi_\\theta(a_t|s_t)\\sum_{t' \\ge t} r_{t'}\\Big].\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reward-to-Go for the Cartpole Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to implement the reward-to-go policy gradient for the ``CartPole`` example, we only need to change the method called at the end of each episode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def finish_episode():\n",
    "    \"\"\"Apply optimizer to policy after each episode\n",
    "    \"\"\"\n",
    "    \n",
    "    R = 0\n",
    "    policy_loss = []\n",
    "    returns = []\n",
    "    \n",
    "    for r in policy.rewards[::-1]:\n",
    "        R = r + GAMMA * R\n",
    "        \n",
    "        returns.insert(0, R)\n",
    "    returns = torch.tensor(returns)\n",
    "    \n",
    "    for log_prob, R in zip(policy.saved_log_probs, \n",
    "                          returns):\n",
    "        policy_loss.append(-log_prob * R)\n",
    "        \n",
    "    optimizer.zero_grad()\n",
    "    policy_loss = torch.cat(policy_loss).sum()\n",
    "    policy_loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    del policy.rewards[:]\n",
    "    del policy.saved_log_probs[:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that this modification makes the training substantially more effective."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baselines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By the policy-gradient theorem, \n",
    "$$\\sum_{t < T} \\nabla_\\theta \\log \\pi_\\theta(a_t|s_t)\\sum_{t' \\ge t} r_{t'}$$\n",
    "is an unbiased estimate for the gradient of the expected total rewards with respect to the policy parameters. However, as we have seen in the simulations, the variance can be substantial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A powerful method to reduce the variance is to correct the above estimator by a **baseline** $b(s)$. That is, we now perform gradient descent with respect to expressions of the form\n",
    "$$\\sum_{t < T} \\nabla_\\theta \\log \\pi_\\theta(a_t|s_t)\\Big(\\sum_{t' \\ge t} r_{t'} - b(s_t)\\Big).$$\n",
    "As long as the baseline $b$ depends only on the state, the resulting estimator is still unbiased. Indeed, for any $t < T$,\n",
    "\\begin{align*}\n",
    "\\mathbb E_{\\pi_\\theta}[ \\nabla_\\theta \\log \\pi_\\theta(a_t|s_t) b(s_t)] = \\int \\Big(\\int \\nabla_\\theta \\log \\pi_\\theta(a_t|s_t)   \\pi_\\theta(a_t|s_t)\\mathrm d a_t\\Big) p_\\theta(s_t) b(s_t) \\mathrm d s_t.\n",
    "\\end{align*}\n",
    "and\n",
    "\\begin{align*}\n",
    "\\int \\nabla_\\theta \\log \\pi_\\theta(a_t|s_t)   \\pi_\\theta(a_t|s_t)\\mathrm d a_t=\\int \\nabla_\\theta  \\pi_\\theta(a_t|s_t)  \\mathrm d a_t  =   \\nabla_\\theta 1= 0.\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since there is substantial freedom in selecting the baseline $b(s)$, this raises the question on what would be an optimal choice. In order to achieve the largest amount of variance reduction, we should define $b(s_t)$ as the conditional expectation of the reward-to-go $\\sum_{t' \\ge t} r_{t'}$ given $s$.  That is, we put\n",
    "$$b(s_t) = V^{\\pi_\\theta}(s_t) = \\mathbb E_{\\pi_\\theta}\\Big[\\sum_{t' \\ge t} r_{t'}\\,\\big|\\,s_t\\Big]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline for the Cartpole Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practice, a crude approximation to this idea would be to subtract the empirical average instead of the conditional mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps = 1e-7\n",
    "def finish_episode():\n",
    "    \"\"\"Apply optimizer to policy after each episode\n",
    "    \"\"\"\n",
    "    \n",
    "    R = 0\n",
    "    policy_loss = []\n",
    "    returns = []\n",
    "    \n",
    "    for r in policy.rewards[::-1]:\n",
    "        R = r + GAMMA * R\n",
    "        returns.insert(0, R)\n",
    "        \n",
    "    returns = torch.tensor(returns)\n",
    "    returns = (returns - returns.mean()) / (returns.std() + eps)\n",
    "    \n",
    "    for log_prob, R in zip(policy.saved_log_probs, returns):\n",
    "        policy_loss.append(-log_prob * R)\n",
    "        \n",
    "    optimizer.zero_grad()\n",
    "    policy_loss = torch.cat(policy_loss).sum()\n",
    "    policy_loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    del policy.rewards[:]\n",
    "    del policy.saved_log_probs[:]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we witness a substantially more stable training in comparison to the setting without baselines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.5\n"
     ]
    }
   ],
   "source": [
    "reinforce(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Gradients with the Q Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, in some settings it also helps to apply the rewards-to-go by the $Q$-function. That is, to have \n",
    "$$\\nabla_\\theta J(\\pi_\\theta) = \\mathbb E_{\\tau \\sim \\pi_\\theta}\\Big[\\sum_{t < T}Q^{\\pi_\\theta}(s_t, a_t) \\nabla_\\theta \\log \\pi_\\theta(a_t|s_t)\\Big]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In other words, we want to show that for every $t < T$,\n",
    "$$\\mathbb E_{\\tau \\sim \\pi_\\theta}\\Big[\\nabla_\\theta \\log \\pi_\\theta(a_t|s_t)\\sum_{t' \\ge t} r_{t'}\\Big]  = \\mathbb E_{\\tau \\sim \\pi_\\theta}\\big[Q^{\\pi_\\theta}(s_t, a_t) \\nabla_\\theta \\log \\pi_\\theta(a_t|s_t)\\big].$$\n",
    "To achieve this goal, we condition on $s_t, a_t$. Then,\n",
    "\\begin{align*}\n",
    "\\mathbb E_{\\tau \\sim \\pi_\\theta}\\Big[\\nabla_\\theta \\log \\pi_\\theta(a_t|s_t)\\sum_{t' \\ge t} r_{t'}\\Big] &= \\mathbb E_{\\tau \\sim \\pi_\\theta}\\Big[\\mathbb E_{\\tau \\sim \\pi_\\theta}\\Big[\\sum_{t' \\ge t} r_{t'}\\Big|s_t,a_t\\Big]\\nabla_\\theta \\log \\pi_\\theta(a_t|s_t)\\Big]= \\mathbb E_{\\tau \\sim \\pi_\\theta}\\Big[Q^{\\pi_\\theta}(s_t, a_t)\\nabla_\\theta \\log \\pi_\\theta(a_t|s_t)\\Big], \n",
    "\\end{align*}\n",
    "as asserted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we can use the value function $V^{\\pi_\\theta}$ as a baseline and obtain the policy gradient theorem for the **advantage function $A^{\\pi_\\theta}(s, a) = Q^{\\pi_\\theta}(s, a) - V^{\\pi_\\theta}(s)$**. That is,\n",
    "$$\\nabla_\\theta J(\\pi_\\theta) = \\mathbb E_{\\tau \\sim \\pi_\\theta}\\Big[\\sum_{t < T}A^{\\pi_\\theta}(s_t, a_t) \\nabla_\\theta \\log \\pi_\\theta(a_t|s_t)\\Big].$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
