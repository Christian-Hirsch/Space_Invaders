{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch\n",
    "\n",
    "In this notebook we will learn how to train a neural network with [pytorch](https://pytorch.org/docs/stable/index.html), a great Python library for Deep Learning.\n",
    "\n",
    "As an example, we want to learn the following mysterious function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def mysterious_function(x):\n",
    "    return torch.sin(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a small function to generate a batch of size ``num_samples``:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.3695],\n",
      "        [-0.8340],\n",
      "        [ 0.3434],\n",
      "        [-0.8488],\n",
      "        [-0.2668],\n",
      "        [ 1.5321],\n",
      "        [ 0.1218],\n",
      "        [ 0.4690],\n",
      "        [ 0.6371],\n",
      "        [-0.9852]])\n",
      "tensor([[ 0.3612],\n",
      "        [-0.7406],\n",
      "        [ 0.3367],\n",
      "        [-0.7505],\n",
      "        [-0.2637],\n",
      "        [ 0.9993],\n",
      "        [ 0.1215],\n",
      "        [ 0.4520],\n",
      "        [ 0.5949],\n",
      "        [-0.8334]])\n"
     ]
    }
   ],
   "source": [
    "def getdata(num_samples):\n",
    "    x = torch.randn(num_samples,1)\n",
    "    y = mysterious_function(x)\n",
    "    return x, y\n",
    "\n",
    "x, y = getdata(10)\n",
    "print(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1.5795, -0.5325,  2.7610, -0.8925,  1.0439,  2.7233, -0.4860, -0.9887,\n",
       "         0.4140, -1.6757])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.randn(10) + torch.randn(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remark**: The variables x and y in this function have the data type ``torch.Tensor``. Tensors are in most aspects very similar to numpy arrays, but they can also store gradients. More on this later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we define the class with our neural network. This class should always inherit ``nn.Module`` (the base class for all neural networks) and has to define the two functions ``__init__`` and ``forward``.\n",
    "\n",
    "The net we are using here contains one linear layer with 1 input and 20 output nodes, followed by a ReLU layer and finally another linear layer with 20 input nodes and 1 output node. We use ``add_module`` to add these layers to our net with the first parameter being the name of the layer added.\n",
    "\n",
    "The ``forward`` function describes the forward pass through the network. In our case we just call the layers defined in ``__init__`` in the right order.\n",
    "\n",
    "(**Tip**: In this case we could also have saved the definition of the class and instead used ``nn.Sequential``, but we do it anyways for educational purposes.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyNet, self).__init__()\n",
    "        self.add_module('hidden', nn.Linear(1, 20))\n",
    "        self.add_module('relu', nn.ReLU())\n",
    "        self.add_module('out', nn.Linear(20,1))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.hidden(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.out(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now comes the heart of our little program - the training loop.\n",
    "For this we first need to choose an optimizing strategy and a loss function (here SGD and L1).\n",
    "And this is what it looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4040],\n",
       "        [0.1771],\n",
       "        [0.6640],\n",
       "        [0.8597],\n",
       "        [0.4944],\n",
       "        [0.5188],\n",
       "        [0.1723],\n",
       "        [0.1834],\n",
       "        [0.1983],\n",
       "        [0.2944]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = MyNet()\n",
    "model.forward(torch.randn(10,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(num_steps, learning_rate, batch_size):\n",
    "    model = MyNet()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate)\n",
    "    loss_fn = nn.MSELoss()\n",
    "    \n",
    "    for step in range(num_steps):\n",
    "        optimizer.zero_grad()\n",
    "        x, y = getdata(batch_size)\n",
    "        y_model = model(x)\n",
    "        loss = loss_fn(y_model, y)\n",
    "        print(loss)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(num_steps, learning_rate, batch_size):\n",
    "    model = MyNet()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate)\n",
    "    loss_fn = nn.L1Loss()\n",
    "    model.train()\n",
    "\n",
    "    for step in range(num_steps):\n",
    "        optimizer.zero_grad()\n",
    "        x, y = getdata(batch_size)\n",
    "        y_model = model(x)\n",
    "        loss = loss_fn(y_model, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, let us go through this part step by step:\n",
    "\n",
    "First we pick an optimization strategy and a loss function (here SGD and L1). Here, ``model.parameters()`` returns an iterator over all parameters of ``model``. (By the way, this is why we used ``add_module`` instead of ``self.hidden = ...`` etc. earlier.)\n",
    "\n",
    "``optimizer.zero_grad()`` initializes all gradients of all parameters to zero.\n",
    "\n",
    "``model(x)`` calls the ``forward`` function of ``model`` and at the same time builds the computation graph dynamically.\n",
    "\n",
    "``loss.backward()`` then iterates backwards through the computation graph and computes the gradients (which are stored in the model parameters).\n",
    "\n",
    "And finally ``optimizer.step()`` executes one learning step with the current gradients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now set our hyperparameters and call our training function.\n",
    "We also added a small test output for good measure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.2305, grad_fn=<MseLossBackward>)\n",
      "tensor(0.1966, grad_fn=<MseLossBackward>)\n",
      "tensor(0.1160, grad_fn=<MseLossBackward>)\n",
      "tensor(0.1295, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0762, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0492, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0655, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0729, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0317, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0309, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0288, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0402, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0411, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0516, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0167, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0176, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0301, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0359, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0285, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0229, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0191, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0241, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0176, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0291, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0190, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0285, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0197, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0181, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0353, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0328, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0239, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0389, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0342, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0738, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0365, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0274, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0743, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0171, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0356, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0162, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0215, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0187, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0183, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0369, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0261, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0212, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0155, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0126, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0269, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0227, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0119, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0293, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0153, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0155, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0452, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0311, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0176, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0363, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0129, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0212, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0459, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0131, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0156, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0280, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0226, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0384, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0270, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0315, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0314, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0434, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0348, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0295, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0381, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0227, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0213, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0287, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0107, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0189, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0125, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0269, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0074, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0358, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0320, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0144, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0291, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0219, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0238, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0186, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0320, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0214, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0277, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0102, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0464, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0242, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0269, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0183, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0240, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0528, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0178, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0247, grad_fn=<MseLossBackward>)\n",
      "mysterious_function(1.3019319772720337) = 0.9640731811523438, but MyNet says 0.8065319061279297 which is wrong by 0.15754127502441406.\n",
      "mysterious_function(-0.18531018495559692) = -0.18425141274929047, but MyNet says -0.17257937788963318 which is wrong by 0.011672034859657288.\n",
      "mysterious_function(1.0357341766357422) = 0.8602369427680969, but MyNet says 0.7116668820381165 which is wrong by 0.14857006072998047.\n",
      "mysterious_function(1.5524685382843018) = 0.999832034111023, but MyNet says 0.8997153043746948 which is wrong by 0.10011672973632812.\n",
      "mysterious_function(1.734289526939392) = 0.9866647124290466, but MyNet says 0.9642265439033508 which is wrong by 0.0224381685256958.\n",
      "mysterious_function(-0.7181773781776428) = -0.6580133438110352, but MyNet says -0.596586287021637 which is wrong by 0.06142705678939819.\n",
      "mysterious_function(0.9128597974777222) = 0.7912557125091553, but MyNet says 0.6708072423934937 which is wrong by 0.12044847011566162.\n",
      "mysterious_function(-0.5981283187866211) = -0.5630967020988464, but MyNet says -0.5331883430480957 which is wrong by 0.029908359050750732.\n",
      "mysterious_function(0.6107736229896545) = 0.5735014081001282, but MyNet says 0.515632152557373 which is wrong by 0.05786925554275513.\n",
      "mysterious_function(-0.5944694876670837) = -0.5600693225860596, but MyNet says -0.5305616855621338 which is wrong by 0.02950763702392578.\n",
      "mysterious_function(-1.0088520050048828) = -0.8462207317352295, but MyNet says -0.7411577701568604 which is wrong by 0.10506296157836914.\n",
      "mysterious_function(-0.08753875643014908) = -0.08742699772119522, but MyNet says -0.08541072905063629 which is wrong by 0.0020162686705589294.\n",
      "mysterious_function(-0.5802822113037109) = -0.548259973526001, but MyNet says -0.5203768014907837 which is wrong by 0.027883172035217285.\n",
      "mysterious_function(-0.1157936230301857) = -0.11553503572940826, but MyNet says -0.11060154438018799 which is wrong by 0.004933491349220276.\n",
      "mysterious_function(0.7659260034561157) = 0.6932047009468079, but MyNet says 0.6179547309875488 which is wrong by 0.07524996995925903.\n",
      "mysterious_function(-0.057567596435546875) = -0.0575358048081398, but MyNet says -0.05868983268737793 which is wrong by 0.0011540278792381287.\n",
      "mysterious_function(-1.3303333520889282) = -0.9712278246879578, but MyNet says -0.8450236916542053 which is wrong by 0.12620413303375244.\n",
      "mysterious_function(0.6775022745132446) = 0.6268488764762878, but MyNet says 0.5596394538879395 which is wrong by 0.06720942258834839.\n",
      "mysterious_function(0.8678892254829407) = 0.7629661560058594, but MyNet says 0.655853271484375 which is wrong by 0.10711288452148438.\n",
      "mysterious_function(3.0927646160125732) = 0.048808638006448746, but MyNet says 1.4462230205535889 which is wrong by 1.3974143266677856.\n",
      "mysterious_function(-0.13941030204296112) = -0.13895916938781738, but MyNet says -0.13165712356567383 which is wrong by 0.007302045822143555.\n",
      "mysterious_function(-0.7342943549156189) = -0.6700634956359863, but MyNet says -0.6046023368835449 which is wrong by 0.0654611587524414.\n",
      "mysterious_function(0.4713609516620636) = 0.45409923791885376, but MyNet says 0.40878966450691223 which is wrong by 0.04530957341194153.\n",
      "mysterious_function(0.3776071071624756) = 0.36869722604751587, but MyNet says 0.33515045046806335 which is wrong by 0.033546775579452515.\n",
      "mysterious_function(-0.09016253799200058) = -0.0900404304265976, but MyNet says -0.08775000274181366 which is wrong by 0.0022904276847839355.\n",
      "mysterious_function(1.5121701955795288) = 0.9982819557189941, but MyNet says 0.8854172229766846 which is wrong by 0.11286473274230957.\n",
      "mysterious_function(-0.5392282605171204) = -0.5134739279747009, but MyNet says -0.4860020875930786 which is wrong by 0.027471840381622314.\n",
      "mysterious_function(-0.7221837639808655) = -0.6610248684883118, but MyNet says -0.598578929901123 which is wrong by 0.06244593858718872.\n",
      "mysterious_function(0.7609451413154602) = 0.6896061897277832, but MyNet says 0.614669919013977 which is wrong by 0.07493627071380615.\n",
      "mysterious_function(0.6181380748748779) = 0.5795187950134277, but MyNet says 0.520488977432251 which is wrong by 0.05902981758117676.\n",
      "mysterious_function(-0.3461305499076843) = -0.33926039934158325, but MyNet says -0.315959632396698 which is wrong by 0.023300766944885254.\n",
      "mysterious_function(-0.4177914559841156) = -0.4057428538799286, but MyNet says -0.37913328409194946 which is wrong by 0.026609569787979126.\n",
      "mysterious_function(-0.631770133972168) = -0.5905741453170776, but MyNet says -0.5536103248596191 which is wrong by 0.036963820457458496.\n",
      "mysterious_function(-1.7843658924102783) = -0.9772805571556091, but MyNet says -0.9854854345321655 which is wrong by 0.008204877376556396.\n",
      "mysterious_function(-0.13614769279956818) = -0.13572748005390167, but MyNet says -0.1287483274936676 which is wrong by 0.00697915256023407.\n",
      "mysterious_function(0.7013369798660278) = 0.6452397108078003, but MyNet says 0.5753583908081055 which is wrong by 0.06988131999969482.\n",
      "mysterious_function(0.9944870471954346) = 0.8384795188903809, but MyNet says 0.6979507207870483 which is wrong by 0.14052879810333252.\n",
      "mysterious_function(1.1224054098129272) = 0.9011458158493042, but MyNet says 0.7404875755310059 which is wrong by 0.16065824031829834.\n",
      "mysterious_function(0.6285529732704163) = 0.5879749059677124, but MyNet says 0.5273575186729431 which is wrong by 0.06061738729476929.\n",
      "mysterious_function(0.955122172832489) = 0.8163843154907227, but MyNet says 0.6848608255386353 which is wrong by 0.1315234899520874.\n",
      "mysterious_function(-0.14635039865970612) = -0.14582853019237518, but MyNet says -0.13784462213516235 which is wrong by 0.00798390805721283.\n",
      "mysterious_function(1.886666178703308) = 0.9505265355110168, but MyNet says 1.0182908773422241 which is wrong by 0.06776434183120728.\n",
      "mysterious_function(0.34688669443130493) = 0.3399716019630432, but MyNet says 0.31102102994918823 which is wrong by 0.02895057201385498.\n",
      "mysterious_function(0.04312845319509506) = 0.04311508312821388, but MyNet says 0.043095335364341736 which is wrong by 1.9747763872146606e-05.\n",
      "mysterious_function(-0.25136351585388184) = -0.24872486293315887, but MyNet says -0.23146957159042358 which is wrong by 0.01725529134273529.\n",
      "mysterious_function(1.485788106918335) = 0.9963889718055725, but MyNet says 0.8760566115379333 which is wrong by 0.12033236026763916.\n",
      "mysterious_function(0.9672026038169861) = 0.8233011364936829, but MyNet says 0.6888779401779175 which is wrong by 0.13442319631576538.\n",
      "mysterious_function(-0.8493767976760864) = -0.7508689761161804, but MyNet says -0.6618403196334839 which is wrong by 0.08902865648269653.\n",
      "mysterious_function(0.7791412472724915) = 0.7026686668395996, but MyNet says 0.6262212991714478 which is wrong by 0.07644736766815186.\n",
      "mysterious_function(0.10103597491979599) = 0.10086416453123093, but MyNet says 0.10170549154281616 which is wrong by 0.0008413270115852356.\n",
      "mysterious_function(-0.40695881843566895) = -0.3958183526992798, but MyNet says -0.36960017681121826 which is wrong by 0.026218175888061523.\n",
      "mysterious_function(0.29569974541664124) = 0.29140928387641907, but MyNet says 0.2708160877227783 which is wrong by 0.020593196153640747.\n",
      "mysterious_function(-0.3551374077796936) = -0.3477191925048828, but MyNet says -0.32398974895477295 which is wrong by 0.023729443550109863.\n",
      "mysterious_function(0.22310814261436462) = 0.22126179933547974, but MyNet says 0.209867924451828 which is wrong by 0.011393874883651733.\n",
      "mysterious_function(2.7496256828308105) = 0.3820069432258606, but MyNet says 1.3244750499725342 which is wrong by 0.9424681067466736.\n",
      "mysterious_function(0.5560717582702637) = 0.5278539061546326, but MyNet says 0.47532594203948975 which is wrong by 0.05252796411514282.\n",
      "mysterious_function(-0.6926252841949463) = -0.6385596990585327, but MyNet says -0.5838775634765625 which is wrong by 0.054682135581970215.\n",
      "mysterious_function(0.5500866770744324) = 0.5227611064910889, but MyNet says 0.4706249535083771 which is wrong by 0.05213615298271179.\n",
      "mysterious_function(0.695580780506134) = 0.6408314108848572, but MyNet says 0.5715621709823608 which is wrong by 0.06926923990249634.\n",
      "mysterious_function(-0.9031046032905579) = -0.7852529883384705, but MyNet says -0.6885626912117004 which is wrong by 0.09669029712677002.\n",
      "mysterious_function(-2.3806655406951904) = -0.6895931363105774, but MyNet says -1.1958725452423096 which is wrong by 0.5062794089317322.\n",
      "mysterious_function(0.029022568836808205) = 0.029018495231866837, but MyNet says 0.028669089078903198 which is wrong by 0.0003494061529636383.\n",
      "mysterious_function(0.26945042610168457) = 0.2662017345428467, but MyNet says 0.25008460879325867 which is wrong by 0.016117125749588013.\n",
      "mysterious_function(0.27268555760383606) = 0.26931872963905334, but MyNet says 0.2527395784854889 which is wrong by 0.016579151153564453.\n",
      "mysterious_function(0.027508748695254326) = 0.02750527858734131, but MyNet says 0.027131736278533936 which is wrong by 0.00037354230880737305.\n",
      "mysterious_function(-0.7213595509529114) = -0.6604061722755432, but MyNet says -0.5981689691543579 which is wrong by 0.0622372031211853.\n",
      "mysterious_function(-0.8383795022964478) = -0.7435604929924011, but MyNet says -0.6563706398010254 which is wrong by 0.08718985319137573.\n",
      "mysterious_function(-1.504215121269226) = -0.9977842569351196, but MyNet says -0.8988165855407715 which is wrong by 0.09896767139434814.\n",
      "mysterious_function(-1.111977458000183) = -0.8965762257575989, but MyNet says -0.7774720788002014 which is wrong by 0.11910414695739746.\n",
      "mysterious_function(-0.5627070665359497) = -0.5334778428077698, but MyNet says -0.5066642761230469 which is wrong by 0.0268135666847229.\n",
      "mysterious_function(0.993349552154541) = 0.8378591537475586, but MyNet says 0.6975725889205933 which is wrong by 0.14028656482696533.\n",
      "mysterious_function(0.23389603197574615) = 0.23176921904087067, but MyNet says 0.21922987699508667 which is wrong by 0.012539342045783997.\n",
      "mysterious_function(0.17199765145778656) = 0.17115086317062378, but MyNet says 0.165513277053833 which is wrong by 0.0056375861167907715.\n",
      "mysterious_function(-0.3342626690864563) = -0.328072726726532, but MyNet says -0.3053787648677826 which is wrong by 0.02269396185874939.\n",
      "mysterious_function(0.024196553975343704) = 0.02419419214129448, but MyNet says 0.023768067359924316 which is wrong by 0.00042612478137016296.\n",
      "mysterious_function(0.5399094820022583) = 0.5140583515167236, but MyNet says 0.46263134479522705 which is wrong by 0.05142700672149658.\n",
      "mysterious_function(-0.4956696629524231) = -0.475620836019516, but MyNet says -0.4476689100265503 which is wrong by 0.027951925992965698.\n",
      "mysterious_function(-0.4451020359992981) = -0.4305499792098999, but MyNet says -0.40316760540008545 which is wrong by 0.027382373809814453.\n",
      "mysterious_function(-0.7292055487632751) = -0.6662774085998535, but MyNet says -0.6020712852478027 which is wrong by 0.06420612335205078.\n",
      "mysterious_function(-0.6303153038024902) = -0.589399516582489, but MyNet says -0.5528867244720459 which is wrong by 0.036512792110443115.\n",
      "mysterious_function(-1.235679268836975) = -0.9443718194961548, but MyNet says -0.8157410621643066 which is wrong by 0.12863075733184814.\n",
      "mysterious_function(-0.28811097145080566) = -0.28414157032966614, but MyNet says -0.26423195004463196 which is wrong by 0.01990962028503418.\n",
      "mysterious_function(0.4616803228855133) = 0.44545313715934753, but MyNet says 0.4011859893798828 which is wrong by 0.04426714777946472.\n",
      "mysterious_function(1.4231594800949097) = 0.9891214370727539, but MyNet says 0.8538355827331543 which is wrong by 0.1352858543395996.\n",
      "mysterious_function(0.4813295900821686) = 0.46295809745788574, but MyNet says 0.41661959886550903 which is wrong by 0.04633849859237671.\n",
      "mysterious_function(0.9131919741630554) = 0.791458785533905, but MyNet says 0.6709176301956177 which is wrong by 0.12054115533828735.\n",
      "mysterious_function(2.900540828704834) = 0.23872417211532593, but MyNet says 1.3780207633972168 which is wrong by 1.139296531677246.\n",
      "mysterious_function(-0.63731849193573) = -0.5950424671173096, but MyNet says -0.5563699007034302 which is wrong by 0.038672566413879395.\n",
      "mysterious_function(-0.29992344975471497) = -0.29544708132743835, but MyNet says -0.27476346492767334 which is wrong by 0.020683616399765015.\n",
      "mysterious_function(-0.5442090630531311) = -0.5177415609359741, but MyNet says -0.49038535356521606 which is wrong by 0.027356207370758057.\n",
      "mysterious_function(0.16083654761314392) = 0.16014401614665985, but MyNet says 0.15582746267318726 which is wrong by 0.004316553473472595.\n",
      "mysterious_function(0.9208151698112488) = 0.7960951924324036, but MyNet says 0.6734527349472046 which is wrong by 0.12264245748519897.\n",
      "mysterious_function(-0.8520863056182861) = -0.7526556849479675, but MyNet says -0.6631879210472107 which is wrong by 0.08946776390075684.\n",
      "mysterious_function(-0.8330439925193787) = -0.7399822473526001, but MyNet says -0.6537169814109802 which is wrong by 0.08626526594161987.\n",
      "mysterious_function(0.42607900500297546) = 0.41330355405807495, but MyNet says 0.37322282791137695 which is wrong by 0.040080726146698.\n",
      "mysterious_function(0.3848871886730194) = 0.3754545748233795, but MyNet says 0.34086862206459045 which is wrong by 0.03458595275878906.\n",
      "mysterious_function(-0.13182009756565094) = -0.13143867254257202, but MyNet says -0.12489005923271179 which is wrong by 0.0065486133098602295.\n",
      "mysterious_function(-0.15646091103553772) = -0.15582333505153656, but MyNet says -0.14685869216918945 which is wrong by 0.008964642882347107.\n",
      "mysterious_function(-0.3439486622810364) = -0.3372071087360382, but MyNet says -0.31401437520980835 which is wrong by 0.02319273352622986.\n",
      "mysterious_function(-0.3284142315387726) = -0.3225424289703369, but MyNet says -0.30016452074050903 which is wrong by 0.02237790822982788.\n",
      "Average loss is 0.09184921532869339.\n"
     ]
    }
   ],
   "source": [
    "num_steps = 100\n",
    "learning_rate = 0.01\n",
    "batch_size = 256\n",
    "\n",
    "model = train(num_steps, learning_rate, batch_size)\n",
    "\n",
    "model.eval()\n",
    "x, y = getdata(100)\n",
    "y_model = model(x)\n",
    "for i in range(100):\n",
    "    print('mysterious_function({}) = {}, but MyNet says {} which is wrong by {}.'.format(\n",
    "        float(x[i]),\n",
    "        float(y[i]),\n",
    "        float(y_model[i]),\n",
    "        float((y[i] - y_model[i]).abs())\n",
    "        ))\n",
    "print('Average loss is {}.'.format(float((y - y_model).abs().mean())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "data = torch.cat([x, y, y_model], dim = 1).detach().numpy()\n",
    "indices = data[:, 0].argsort(0)\n",
    "data = np.array([data[i, :] for i in indices])\n",
    "data = data.T\n",
    "plt.plot(data[0], data[1], '-o', label='data')\n",
    "plt.plot(data[0], data[2], '-o', label='prediction')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
