{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  TD($\\lambda$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, study the TD($\\lambda$) as a model-free RL algorithm that can interprolate between TD$(0)$ and MC. We first explain the mathematical concept behind it and implement it via eligibility traces."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the theory part, we follow the lucid [monograph by Richard Sutton and Andrew Barto](http://incompleteideas.net/book/the-book-2nd.html), whereas the implementation relies on "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/mpatacchiola/dissecting-reinforcement-learning/blob/master/src/3/temporal_differencing_prediction_trace.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/jiexunsee/Deep-Watkins-Q-and-Actor-Critic/blob/master/DeepTDLambdaLearner.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we load the ``Frozen-Lake`` environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Right)\n",
      "S\u001b[41mF\u001b[0mFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "\n",
    "%matplotlib inline \n",
    "\n",
    "env = gym.make('FrozenLake-v0')\n",
    "\n",
    "s = env.reset()\n",
    "env.render()\n",
    "env.step(env.action_space.sample())\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we define the single steps in the $\\mathsf{TD}(\\lambda)$-scheme. The goal in this toy problem is to estimate the value function associated with taking random actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "LR = .8\n",
    "GAMMA = .99\n",
    "LAMBDA = .5\n",
    "\n",
    "def q_step(s, i):\n",
    "    #get random action\n",
    "    a = np.random.randint(env.nA)\n",
    "    \n",
    "    #Get new state and reward from environment\n",
    "    ss, r, d, _ = env.step(a)\n",
    "    \n",
    "    #update eligibility\n",
    "    global trace\n",
    "    global V\n",
    "    trace *= GAMMA * LAMBDA\n",
    "    trace[s] += 1\n",
    "    \n",
    "    #update value\n",
    "    delta = r + GAMMA * V[ss] - V[s]\n",
    "    V += LR * delta * trace\n",
    "    \n",
    "    s = ss\n",
    "    return s, r, d    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we let the algorithm run and print the learned state-value function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "NEPS = int(5e3)\n",
    "rList = []\n",
    "V = np.zeros(env.nS)\n",
    "\n",
    "for i in range(NEPS):\n",
    "    #Reset environment and get first new observation\n",
    "    s = env.reset()\n",
    "    trace = np.zeros(env.nS)\n",
    "    rAll = 0\n",
    "    \n",
    "    #The Q-Table learning algorithm\n",
    "    while True:\n",
    "        s, r, d = q_step(s, i)\n",
    "        rAll += r\n",
    "        if(d):\n",
    "            break\n",
    "    rList.append(rAll)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.00708151  0.01044591  0.00465903  0.00310812  0.00056628  0.\n",
      "  0.00196414  0.          0.00329716  0.02576031  0.00958781  0.\n",
      "  0.         -0.00784054  0.36166087  0.        ]\n"
     ]
    }
   ],
   "source": [
    "print(V)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $\\mathsf{TD}(\\lambda)$ for Optimal Policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we modify the $\\mathsf{TD}(\\lambda)$ algorithm to estimate the optimal policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "LR = .8\n",
    "GAMMA = .99\n",
    "LAMBDA = .5\n",
    "EPS  = .05\n",
    "\n",
    "def q_step_opt(s, i):\n",
    "    \n",
    "    #update eligibility\n",
    "    global q_trace\n",
    "    global Q\n",
    "    \n",
    "    #select action epsilon-greedy\n",
    "    a = np.argmax(Q[s,:] + np.random.randn(1, env.nA) * (EPS  + 1  / (i + 1)))\n",
    "    \n",
    "    #Get new state and reward from environment\n",
    "    ss, r, d, _ = env.step(a)\n",
    "    \n",
    "    q_trace *= GAMMA * LAMBDA\n",
    "    q_trace[s, a] += 1\n",
    "    \n",
    "    #update value\n",
    "    delta = r + GAMMA * np.max(Q[ss, :]) - Q[s, a]\n",
    "    Q += LR * delta * q_trace\n",
    "    \n",
    "    s = ss\n",
    "    return s, r, d "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we perform the fitting in several epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "NEPS = int(1e4)\n",
    "rList = []\n",
    "Q = np.zeros((env.nS, env.nA))\n",
    "\n",
    "for i in range(NEPS):\n",
    "    #Reset environment and get first new observation\n",
    "    s = env.reset()\n",
    "    q_trace = np.zeros((env.nS, env.nA))\n",
    "    rAll = 0\n",
    "    \n",
    "    #The Q-Table learning algorithm\n",
    "    while True:\n",
    "        s, r, d = q_step_opt(s, i)\n",
    "        rAll += r\n",
    "        if(d):\n",
    "            break\n",
    "    rList.append(rAll)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
