{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actor-Critic Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the [previous notebook](./reinforce.ipynb), we have encountered the [REINFORCE algorithm]() as a powerful alternative to [deep Q learning](). It is based directly on the idea of finding a parametric policy that maximizes the total expected rewards. In order to reduce the high variance of Monte Carlo samples for the gradients, we also discussed the use of baselines. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although, we identified the state-value function $V$ as the optimal baseline, we did not comment on how to compute it in practice. In this notebook, we discuss the **Actor-Critic Method** to address this problem. Loosely speaking, we extend the architecture of the REINFORCE algorithm with a second deep network that is used to estimate the value function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The networks are learned simultaneously as the agent proceeds through the environment. The policy network from the REINFORCE algorithm is called **Actor** as it suggests which action to take when in a given state. The second network is called **Critic** as it evaluates the steps by the actor through estimating the value function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we build on the code samples from the amazing [pytorch-examples repository](https://github.com/pytorch/examples/blob/master/reinforcement_learning/actor_critic.py) and follow closely the highly instructive conceptual derivations from [OpenAI spinning up](https://spinningup.openai.com/en/latest/algorithms/sac.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Pseudocode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As indicated above, the Actor-Critic Method consists of two central ingredients. First, a policy-network $\\pi_\\theta(s)$ returning the probability of selecting an action when starting from a state $s$. Additionally, we now have a value network $V_\\phi(s)$ approximating the value function $V^{\\pi_\\theta}(s)$ under the policy $\\pi_\\theta$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To fit policy parameters $\\theta$, we perform a gradient step as in the REINFORCE algorithm with the critic $V_\\phi$ as baseline:\n",
    "$$\\theta \\leftarrow \\theta + \\alpha \\sum_{t < T} \\nabla_\\theta \\log \\pi_\\theta(a_t|s_t)\\Big(\\sum_{t' \\ge t} r_{t'} - V_\\phi(s_t)\\Big)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To fit the critic parameters $\\phi$ we perform a gradient step obtained from regressing the critic $V_\\varphi(s_t)$ against the rewards-to-go $\\sum_{t' \\ge t}r_{t'}$ with a suitable loss-function $\\ell$. That is,\n",
    "$$\\phi \\leftarrow \\phi + \\alpha' \\sum_{t < T} \\nabla_\\phi \\ell\\Big(\\sum_{t' \\ge t}r_{t'}, V_\\phi(s_t)\\Big)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cartpole Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we import ``gym`` and load the ``CartPole``-environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "env = gym.make('CartPole-v0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of creating two entirely separate networks for selecting an action and estimating the value-function, we use a common base network with one hidden layer. However, in addition to one layer for selecting an action, the network now contains a second head to estimate the value function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "nactions = env.action_space.n\n",
    "state_dim = env.observation_space.shape[0]\n",
    "nhidden = 128\n",
    "\n",
    "class Policy(nn.Module):\n",
    "    \"\"\"Neural network parametrizing the policy\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize the policy network\n",
    "        \"\"\"\n",
    "        super(Policy, self).__init__()\n",
    "        self.affine = nn.Linear(state_dim, nhidden)\n",
    "        self.action_head = nn.Linear(nhidden, nactions)\n",
    "        self.value_head = nn.Linear(nhidden, 1)\n",
    "\n",
    "        self.saved_actions = []\n",
    "        self.rewards = []\n",
    "\n",
    "    def forward(self, s):\n",
    "        \"\"\"Compute action and state value from state\n",
    "    \n",
    "        # Arguments\n",
    "            s: input state\n",
    "        # Result\n",
    "            suggested action and state value\n",
    "        \"\"\"\n",
    "        s = F.relu(self.affine(s))\n",
    "        action_scores = self.action_head(s)\n",
    "        state_values = self.value_head(s)\n",
    "        return F.softmax(action_scores, dim=-1), state_values\n",
    "\n",
    "\n",
    "model = Policy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The selection of an action proceeds in almost the same way as for the REINFORCE algorithm. The only difference is that we now also record the value function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "from collections import namedtuple\n",
    "\n",
    "def select_action(s):\n",
    "    \"\"\"Select action according to policy\n",
    "    \n",
    "    # Arguments\n",
    "        s: state\n",
    "    # Result\n",
    "        selected action\n",
    "    \"\"\"\n",
    "    s = torch.from_numpy(s).float()\n",
    "    probs, v = model(s)\n",
    "    m = Categorical(probs)\n",
    "    action = m.sample()\n",
    "    \n",
    "    #save action and associated value function\n",
    "    model.saved_actions.append((m.log_prob(action), v))\n",
    "    return action.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also the learning step is very similar.  However, we now also fit the output of the value network to the rewards-to-go."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "GAMMA = .99\n",
    "optimizer = optim.Adam(model.parameters(), \n",
    "                       lr = 1e-2)\n",
    "eps = 1e-7\n",
    "\n",
    "def finish_episode():\n",
    "    \"\"\"Apply optimizer to policy network after each episode\n",
    "    \"\"\"\n",
    "    R = 0\n",
    "    saved_actions = model.saved_actions\n",
    "    policy_losses = []\n",
    "    value_losses = []\n",
    "    returns = []\n",
    "    \n",
    "    #compute to go rewards and standardize\n",
    "    for r in model.rewards[::-1]:\n",
    "        R = r + GAMMA * R\n",
    "        returns.insert(0, R)\n",
    "    returns = torch.tensor(returns)\n",
    "    returns = (returns - returns.mean()) / (returns.std() + eps)\n",
    "    \n",
    "    #compute losses for actor and critic\n",
    "    for (log_prob, value), R in zip(saved_actions, returns):\n",
    "        advantage = R - value.item()\n",
    "        policy_losses.append(-log_prob * advantage)\n",
    "        value_losses.append(F.smooth_l1_loss(value, torch.tensor([R])))\n",
    "        \n",
    "        \n",
    "    optimizer.zero_grad()\n",
    "    loss = torch.stack(policy_losses).sum() + torch.stack(value_losses).sum()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    del model.rewards[:]\n",
    "    del model.saved_actions[:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we iterate over several episodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.15\n",
      "112.55445072558855\n",
      "136.02372568341798\n",
      "178.54495893556654\n",
      "192.908989916315\n",
      "199.38158812970656\n",
      "186.19958292910866\n"
     ]
    }
   ],
   "source": [
    "neps = int(1e3)\n",
    "\n",
    "PRINT_FREQ = int(1e2)\n",
    "MAX_STEPS = int(1e4)\n",
    "\n",
    "running_reward = 10\n",
    "\n",
    "for i in range(neps):\n",
    "    s, ep_reward = env.reset(), 0\n",
    "    \n",
    "    #collect rewards\n",
    "    for t in range(1, MAX_STEPS):\n",
    "        a = select_action(s)\n",
    "        s, r, done, _ = env.step(a)\n",
    "        model.rewards.append(r)\n",
    "        ep_reward += r\n",
    "        if done:\n",
    "                        break\n",
    "                \n",
    "    #average rewards\n",
    "    running_reward = 0.05 * ep_reward + (1 - 0.05) * running_reward\n",
    "    \n",
    "    #train actor and critic\n",
    "    finish_episode()\n",
    "    if i % PRINT_FREQ == 0:\n",
    "        print(running_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
