{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Networks\n",
    "\n",
    "In the [previous notebook](./pytorchIntro.ipynb) we have seen how you can train a neural network with pytorch. Next we will learn about the torchvision package and how you can use it to classify images. As our challenge for this notebook, we will use the [Dogs vs. Cats](https://www.kaggle.com/c/dogs-vs-cats/data) Kaggle Challenge. (You can ask a Kursleiter for the data set.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as tf\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To handle the loading and sampling of the data for the training we use the torchvision ``ImageFolder`` class. It expects the image data arranged in the following way: For each category, there is a folder in the root folder with the name of the category as name and containing all images of this category.\n",
    "\n",
    "In our case you will need two folders (for test and training), each of them containing a folder ``dog`` and a folder ``cat`` containing the images. Enter the paths of your folders here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# edit me\n",
    "train_folder = '../dogs_vs_cats/train'\n",
    "test_folder = '../dogs_vs_cats/test'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we define a class for our convolutional network. It has 3 convolutional layers and a linear output layer. We also make use of the nn.Sequential module this time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DogCatNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DogCatNet, self).__init__()\n",
    "        \n",
    "        conv_layers = [\n",
    "            # 64 x 64 x 3\n",
    "            nn.Conv2d(in_channels = 3, out_channels = 32, kernel_size = 3, padding = 1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size = 2),\n",
    "            # 32 x 32 x 32\n",
    "            nn.Conv2d(in_channels = 32, out_channels = 64, kernel_size = 3, padding = 1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size = 2),\n",
    "            # 16 x 16 x 64\n",
    "            nn.Conv2d(in_channels = 64, out_channels = 128, kernel_size = 3, padding = 1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size = 2)\n",
    "            # 8 x 8 x 128 = 8192\n",
    "        ]\n",
    "        self.add_module('convolutions', nn.Sequential(*conv_layers))\n",
    "        self.add_module('output', nn.Linear(8192,2))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.convolutions(x)\n",
    "        x = self.output(x.view((x.size()[0], -1)))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more information on the layers (and additional hyperparameters) we refer to the [documentation](https://pytorch.org/docs/stable/nn.html). Also note that we need to change the format of the tensor once we switch from convolutions to full connected layers.\n",
    "\n",
    "Let us now define the ``ImageFolder`` we use for training and testing. The transform option allows us to attach a function that is executed for each image upon loading. For training, we randomly crop the image as a form of data augmentation, resize it to 64x64, then transform it to a tensor and normalize it. (The mean and std parameters are coming from the ImageNet dataset.) For testing, we do not crop the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transformation = tf.Compose([\n",
    "    tf.RandomResizedCrop(64, scale = (0.5, 1.0)),\n",
    "    tf.ToTensor(),\n",
    "    tf.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "train_set = ImageFolder(train_folder, transform = train_transformation)\n",
    "\n",
    "test_transformation = tf.Compose([\n",
    "    tf.Resize([64,64]),\n",
    "    tf.ToTensor(),\n",
    "    tf.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "test_set = ImageFolder(test_folder, transform = test_transformation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For both sets we also define a ``DataLoader`` which is pretty much an iterator over the dataset that also takes care of shuffling, batch_size and some other things:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_set, shuffle = True, batch_size = 32, num_workers = 3)\n",
    "test_loader = DataLoader(test_set, shuffle = True, batch_size = 32, num_workers = 3, drop_last = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The parameter ``num_workers`` describes how many threads the DataLoader will use to load the data. The correct number naturally depends on your hardware. In case you are training with a GPU and you want to achieve optimal performance, this is probably the number of CPU cores on your machine.\n",
    "\n",
    "And this is what a training epoch and a test run would look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-4\n",
    "model = DogCatNet()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)\n",
    "\n",
    "def train_epoch():\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    model.train()\n",
    "    loss_sum = 0\n",
    "    steps = 0\n",
    "    for x, label in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        y = model(x)\n",
    "        loss = loss_fn(y, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loss_sum += float(loss)\n",
    "        steps += 1\n",
    "        print(\"Current loss: {}, accumulated loss: {}\".format(float(loss), loss_sum / steps))\n",
    "    average_loss = loss_sum / steps\n",
    "    return average_loss\n",
    "\n",
    "def test():\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    model.eval()\n",
    "    num_errors = 0\n",
    "    loss_sum = 0\n",
    "    steps = 0\n",
    "    for x, label in test_loader:\n",
    "        y = model(x)\n",
    "        loss = loss_fn(y, label)\n",
    "        num_errors += int((y.max(1)[1] != label).float().sum())\n",
    "        loss_sum += float(loss)\n",
    "        steps += 1\n",
    "    average_loss = loss_sum / steps\n",
    "    return average_loss, num_errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you should be ready to classify some images of dogs and cats. Have fun! :-)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current loss: 0.7055887579917908, accumulated loss: 0.7055887579917908\n",
      "Current loss: 0.6922820806503296, accumulated loss: 0.6989354193210602\n",
      "Current loss: 0.7056407928466797, accumulated loss: 0.7011705438296\n",
      "Current loss: 0.6934418678283691, accumulated loss: 0.6992383748292923\n",
      "Current loss: 0.6891087889671326, accumulated loss: 0.6972124576568604\n",
      "Current loss: 0.7171083092689514, accumulated loss: 0.7005284329255422\n",
      "Current loss: 0.685908854007721, accumulated loss: 0.6984399216515678\n",
      "Current loss: 0.6748577952384949, accumulated loss: 0.6954921558499336\n",
      "Current loss: 0.6953023076057434, accumulated loss: 0.6954710616005791\n",
      "Current loss: 0.6996814608573914, accumulated loss: 0.6958921015262604\n",
      "Current loss: 0.7020387053489685, accumulated loss: 0.6964508836919611\n",
      "Current loss: 0.6883124113082886, accumulated loss: 0.6957726776599884\n",
      "Current loss: 0.6840443015098572, accumulated loss: 0.694870494879209\n",
      "Current loss: 0.689433753490448, accumulated loss: 0.6944821562085833\n",
      "Current loss: 0.692973792552948, accumulated loss: 0.6943815986315409\n",
      "Current loss: 0.697990357875824, accumulated loss: 0.6946071460843086\n",
      "Current loss: 0.6917417645454407, accumulated loss: 0.694438594229081\n",
      "Current loss: 0.6789907217025757, accumulated loss: 0.6935803790887197\n",
      "Current loss: 0.7016546726226807, accumulated loss: 0.6940053419062966\n",
      "Current loss: 0.6829671263694763, accumulated loss: 0.6934534311294556\n",
      "Current loss: 0.6712418794631958, accumulated loss: 0.692395738192967\n",
      "Current loss: 0.7202397584915161, accumulated loss: 0.6936613754792647\n",
      "Current loss: 0.7346585988998413, accumulated loss: 0.6954438634540724\n",
      "Current loss: 0.7078002095222473, accumulated loss: 0.695958711206913\n",
      "Current loss: 0.6865553855895996, accumulated loss: 0.6955825781822205\n",
      "Current loss: 0.6887935996055603, accumulated loss: 0.6953214636215796\n",
      "Current loss: 0.6871166229248047, accumulated loss: 0.6950175806328103\n",
      "Current loss: 0.685657799243927, accumulated loss: 0.6946833027260644\n",
      "Current loss: 0.6876747608184814, accumulated loss: 0.6944416288671822\n",
      "Current loss: 0.6861061453819275, accumulated loss: 0.6941637794176737\n",
      "Current loss: 0.6771042346954346, accumulated loss: 0.693613471523408\n",
      "Current loss: 0.6869245767593384, accumulated loss: 0.6934044435620308\n",
      "Current loss: 0.6836419701576233, accumulated loss: 0.6931086110346245\n",
      "Current loss: 0.6701025366783142, accumulated loss: 0.6924319617888507\n",
      "Current loss: 0.6849929094314575, accumulated loss: 0.6922194174357823\n",
      "Current loss: 0.6934332847595215, accumulated loss: 0.6922531359725528\n",
      "Current loss: 0.688419759273529, accumulated loss: 0.6921495311969036\n",
      "Current loss: 0.6973589062690735, accumulated loss: 0.6922866200145922\n",
      "Current loss: 0.6688509583473206, accumulated loss: 0.6916857056128674\n",
      "Current loss: 0.7075615525245667, accumulated loss: 0.6920826017856598\n",
      "Current loss: 0.687288224697113, accumulated loss: 0.6919656657591099\n",
      "Current loss: 0.6892954707145691, accumulated loss: 0.6919020896866208\n",
      "Current loss: 0.6921351552009583, accumulated loss: 0.6919075098148612\n",
      "Current loss: 0.7092103958129883, accumulated loss: 0.6923007572239096\n",
      "Current loss: 0.6443784832954407, accumulated loss: 0.6912358178032769\n",
      "Current loss: 0.6881235837936401, accumulated loss: 0.6911681605421979\n",
      "Current loss: 0.6777179837226868, accumulated loss: 0.6908819865673146\n",
      "Current loss: 0.701620876789093, accumulated loss: 0.691105713446935\n",
      "Current loss: 0.6920472979545593, accumulated loss: 0.6911249294572946\n",
      "Current loss: 0.673378050327301, accumulated loss: 0.6907699918746948\n",
      "Current loss: 0.6862430572509766, accumulated loss: 0.6906812284507003\n"
     ]
    }
   ],
   "source": [
    "train_result = train_epoch()\n",
    "print(\"Average loss after training epoch: {}\".format(train_result))\n",
    "test_loss, errors = test()\n",
    "print(\"Average test loss: {}\".format(test_loss))\n",
    "print(\"Number of incorrectly classified images: {}\".format(errors))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
